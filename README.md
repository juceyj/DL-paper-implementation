<!--
 * @Author: Jiayi Liu
 * @Date: 2022-10-02 08:25:41
 * @LastEditors: Jiayi Liu
 * @LastEditTime: 2022-11-17 14:14:11
 * @FilePath: /private_jacieliu/DL-paper-implementation/README.md
 * @Description: 
 * Copyright (c) 2022 by JiayiLiu, All Rights Reserved. 
-->
Paper得来终觉浅，绝知此事要coding。

Knowledge obtained on the papers always feels shallow, and it must be known that this thing requires coding.

## Purpose

1. Minimal Practice
2. Project Notes
3. Optimization
4. Algorithm Competition

## Basic

### 1. CNN

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| Resnet  | [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385v1)  | &#10004; | &#10004; |
| InceptionV3  | [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567v3)  | &#10004; | &#10004; |
| InceptionV4  | [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)  | &#10004; | &#10006; |
| MobileNet  | [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)  | &#10006; | &#10006; |
| EfficientNet  | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  | &#10006; | &#10006; |
| Residual Attention Network  | [Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904)  | &#10006; | &#10006; |
| Non-deep Networks  | [Non-deep Networks](https://arxiv.org/abs/2110.07641)  | &#10006; | &#10006; |
### 2. Transformer

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| Transformer  | [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  | &#10004; | &#10004; |
| BERT  | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  | &#10004; | &#10006; |
| GPT-3  | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)  | &#10006; | &#10006; |

### 3. Generation

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| GAN  | [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)  | &#10006; | &#10006; |
| pix2pix  | [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)  | &#10006; | &#10006; |
| CycleGAN  | [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)  | &#10006; | &#10006; |
| Guided Diffusion  | [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)  | &#10006; | &#10006; |
| DALL.E 2  | [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)  | &#10006; | &#10006; |

### 4. Multimodal

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| CLIP  | [-](https://arxiv.org/abs/1406.2661)  | &#10006; | &#10006; |
| ViLT  | [-](https://arxiv.org/abs/1611.07004)  | &#10006; | &#10006; |
| CLIP4clip  | [-](https://arxiv.org/abs/1703.10593)  | &#10006; | &#10006; |
| GroupViT  | [-](https://arxiv.org/abs/2105.05233)  | &#10006; | &#10006; |
| CILPasso  | [-](https://arxiv.org/abs/2204.06125)  | &#10006; | &#10006; |

## Project
### 1. Object Detection

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| R-CNN  | [-](https://arxiv.org/abs/1406.2661)  | &#10006; | &#10006; |
| Faster R-CNN  | [-](https://arxiv.org/abs/1611.07004)  | &#10006; | &#10006; |
| YoloV3  | [You Only Look Once: Unified, Real-time Object Detection](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html)  | &#10004; | &#10006; |
| DETR  | [-](https://arxiv.org/abs/2105.05233)  | &#10006; | &#10006; |

### 3. Audio-visual

| Model   | Link   | Paper  | Code  |
| ----  | ----  | ----  | ----  |
| Syncnet  | [Out of time: automated lip sync in the wild](https://link.springer.com/chapter/10.1007/978-3-319-54427-4_19)  | &#10004; | &#10006; |
| Wav2lip  | [A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild](https://arxiv.org/abs/2008.10010)  | &#10004; | &#10006; |